{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dmis-lab/llama-3-meerkat-8b-v1.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-16 19:18:18 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 02-16 19:18:23 config.py:542] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 02-16 19:18:23 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='dmis-lab/llama-3-meerkat-8b-v1.0', speculative_config=None, tokenizer='dmis-lab/llama-3-meerkat-8b-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=dmis-lab/llama-3-meerkat-8b-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-16 19:18:25 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-16 19:18:25 model_runner.py:1110] Starting to load model dmis-lab/llama-3-meerkat-8b-v1.0...\n",
      "INFO 02-16 19:18:25 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1217d16ac7b3417982a9e1e053675f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-16 19:18:27 model_runner.py:1115] Loading model weights took 14.9595 GB\n",
      "INFO 02-16 19:18:28 worker.py:267] Memory profiling takes 0.55 seconds\n",
      "INFO 02-16 19:18:28 worker.py:267] the current vLLM instance can use total_gpu_memory (47.50GiB) x gpu_memory_utilization (0.90) = 42.75GiB\n",
      "INFO 02-16 19:18:28 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 26.52GiB.\n",
      "INFO 02-16 19:18:28 executor_base.py:110] # CUDA blocks: 13578, # CPU blocks: 2048\n",
      "INFO 02-16 19:18:28 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 106.08x\n",
      "INFO 02-16 19:18:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-16 19:18:42 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.26 GiB\n",
      "INFO 02-16 19:18:42 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 14.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize vLLM with the model\n",
    "llm = LLM(model=model_name,\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=2048,\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.5, top_p=0.95, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "idf = pd.read_csv('medhalt_generic_new.csv')\n",
    "input_df = idf[0:200].copy()\n",
    "print(len(input_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "    \"content\": '''\"The following is a multiple-choice question about medical knowledge. Solve this in\"\n",
    "    \" a step-by-step fashion, starting by summarizing the available information. Output\"\n",
    "    \" a single option from the given options as the final answer. You are strongly\"\n",
    "    \" required to follow the specified output format; conclude your response with the\"\n",
    "    ' phrase \"the answer is ([option_id]) [answer_string]\".\\n\\n'\n",
    "    '''},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, options, answer, message_for_answering):\n",
    "\n",
    "    qa = f\"Question: {question}\\n Options: {options}\\n\"\n",
    "    message_for_answering.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": qa,\n",
    "    })\n",
    "    #print(message_for_answering)\n",
    "    message_for_answering = str(message_for_answering)\n",
    "    \n",
    "    try:\n",
    "        response = llm.generate(message_for_answering, sampling_params)\n",
    "\n",
    "        return response[0].outputs[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs, labels = [], []\n",
    "results = []\n",
    "for index, row in input_df.iterrows():\n",
    "    question_id = row['id']\n",
    "    question = row['question']\n",
    "    option = row['options']\n",
    "    correct_answer = row['correct_answer']\n",
    "    correct_index = row['correct_index']\n",
    "    content = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": messages}, {\"role\": \"user\", \"content\": question + \" \" + option}],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    inputs.append(content)\n",
    "    labels.append(correct_index)\n",
    "    results.append({\n",
    "        \"qid\": question_id,\n",
    "        \"question\": question,\n",
    "        \"options\": row['options'],\n",
    "        \"answer\": correct_answer,\n",
    "    })\n",
    "\n",
    "generated = llm.generate(\n",
    "    inputs,\n",
    "    SamplingParams(\n",
    "        temperature=0.0,\n",
    "        stop_token_ids=[tokenizer.vocab[\"<|eot_id|>\"]],\n",
    "        max_tokens=1024,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(text: str, options: str = \"ABCD\") -> str:\n",
    "    return (re.findall(rf\"he answer is \\(([{options}])\\)\", text) or [options[0]])[-1]\n",
    "\n",
    "def extract_numeric_answer(text: str) -> str:\n",
    "    match = re.findall(r\"he answer is \\((\\d+)\\)\", text)\n",
    "    return match[-1] if match else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness = []\n",
    "\n",
    "for g, l, r in zip(generated, labels, results):\n",
    "    extracted = extract_numeric_answer(g.outputs[0].text)\n",
    "    correctness.append( int(extracted) == int(l))\n",
    "    r['correct_index'] = l\n",
    "    r['reponse_index'] = extracted\n",
    "    r['response'] = g.outputs[0].text\n",
    "    print(l, extracted)\n",
    "    \n",
    "\n",
    "print(sum(correctness) / len(correctness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_df = output_df[output_df[\"correct_index\"].astype(str) != output_df[\"reponse_index\"].astype(str)]\n",
    "output_df.to_csv(\"path_to_save_incorrect_list.csv\")\n",
    "\n",
    "# calculate the performance from the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
